# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o1GhkdnJj0G1QXJ7dLBa-Muj2Gqm-MOx
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from random import sample

# Set the path to the directory containing the image dataset
dataset_path = ""C:\Users\ANURAJ B N\Downloads\train_metadata.csv" "

# Function to load a sample of images from the dataset
def load_sample_images(path, num_samples=5):
    image_files = os.listdir(path)
    sample_images = sample(image_files, num_samples)
    return [cv2.imread(os.path.join(path, img)) for img in sample_images]

# Function to display a random sample of images
def display_images(images, titles=None):
    num_images = len(images)
    fig, axs = plt.subplots(1, num_images, figsize=(15, 5))

    for i in range(num_images):
        axs[i].imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))
        axs[i].axis('off')
        if titles:
            axs[i].set_title(titles[i])

plt.show()

# Load a sample of images
sample_images = load_sample_images(dataset_path)

# Display the sample images
display_images(sample_images, titles=["Image {}".format(i+1) for i in range(len(sample_images))])

from torch.utils.data import DataLoader, random_split
import torch.optim as optim
import torch.nn as nn
from torchvision import transforms
from dataset import CustomMultispectralDataset  # You need to define your dataset class

# Define your dataset and dataloaders
dataset = CustomMultispectralDataset(...)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_dataloader = DataLoader(train_dataset, batch_size=..., shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=...)

# Fine-tune the model
optimizer = optim.AdamW(model.parameters(), lr=...)
criterion = nn.CrossEntropyLoss()

# Training loop
for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    # Validation loop
    model.eval()
    with torch.no_grad():
        for inputs, labels in val_dataloader:
            outputs = model(inputs)
            val_loss = criterion(outputs, labels)

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}")

# Import necessary libraries
import torch
from transformers import ViTFeatureExtractor, ViTForImageClassification
from transformers import DeiTFeatureExtractor, DeiTForImageClassification
from transformers import SwinFeatureExtractor, SwinForImageClassification
from transformers import EfficientNetFeatureExtractor, EfficientNetForImageClassification
from transformers import UNetWithTransformerEncoders, DeepLabV3WithTransformerBackbone

# Function to load a specific pre-trained model
def load_model(model_type):
    if model_type == "ViT":
        feature_extractor = ViTFeatureExtractor("google/vit-base-patch16-224-in21k")
        model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k")
    elif model_type == "DeiT":
        feature_extractor = DeiTFeatureExtractor("facebook/deit-base-distilled-patch16-224")
        model = DeiTForImageClassification.from_pretrained("facebook/deit-base-distilled-patch16-224")
    elif model_type == "Swin":
        feature_extractor = SwinFeatureExtractor("microsoft/swin-base-patch4-window7_224")
        model = SwinForImageClassification.from_pretrained("microsoft/swin-base-patch4-window7_224")
    elif model_type == "EfficientNet":
        feature_extractor = EfficientNetFeatureExtractor("efficientnet/b0")
        model = EfficientNetForImageClassification.from_pretrained("efficientnet/b0")
    elif model_type == "UNetTransformer":
        model = UNetWithTransformerEncoders(...)
    elif model_type == "DeepLabTransformer":
        model = DeepLabV3WithTransformerBackbone(...)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    return feature_extractor, model

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim import lr_scheduler
import torch.quantization as quantization

# Define a simple neural network model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Load MNIST dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform),
                                           batch_size=64, shuffle=True)

# Instantiate the model
model = SimpleModel()

# Pruning
def apply_pruning(model):
    parameters_to_prune = ((model.fc1, 'weight'), (model.fc2, 'weight'))
    pruning_method = torch.nn.utils.prune.L1Unstructured(amount=0.2)
    prune.global_unstructured(parameters_to_prune, pruning_method)

# Quantization
def apply_quantization(model):
    quantized_model = quantization.quantize_dynamic(
        model, {nn.Linear}, dtype=torch.qint8
    )
    return quantized_model

# Inference Optimization (Reducing Time Complexity)
def inference_optimization(model):
    model = model.to('cuda')  # Move model to GPU for faster inference
    model.eval()
    example_input = torch.rand(1, 784).to('cuda')  # Example input
    traced_model = torch.jit.trace(model, example_input)
    return traced_model

# Hardware Acceleration (GPU)
def enable_gpu_acceleration(model):
    model = model.to('cuda')
    return model

# Example usage
apply_pruning(model)
quantized_model = apply_quantization(model)
optimized_model = inference_optimization(quantized_model)
accelerated_model = enable_gpu_acceleration(optimized_model)

from sklearn.metrics import accuracy_score, f1_score
import torch

def evaluate_model(model, dataloader):
    model.eval()
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, predictions = torch.max(outputs, 1)
            all_labels.extend(labels.numpy())
            all_predictions.extend(predictions.numpy())

    accuracy = accuracy_score(all_labels, all_predictions)
    f1 = f1_score(all_labels, all_predictions, average='weighted')

    return accuracy, f1

# Evaluate the original model
accuracy_original, f1_original = evaluate_model(model, test_loader)
print(f"Accuracy (Original): {accuracy_original:.4f}")
print(f"F1 Score (Original): {f1_original:.4f}")

# Evaluate the pruned model
accuracy_pruned, f1_pruned = evaluate_model(model, test_loader)
print(f"Accuracy (Pruned): {accuracy_pruned:.4f}")
print(f"F1 Score (Pruned): {f1_pruned:.4f}")

# Evaluate the quantized model
accuracy_quantized, f1_quantized = evaluate_model(quantized_model, test_loader)
print(f"Accuracy (Quantized): {accuracy_quantized:.4f}")
print(f"F1 Score (Quantized): {f1_quantized:.4f}")

# Evaluate the optimized model
accuracy_optimized, f1_optimized = evaluate_model(optimized_model, test_loader)
print(f"Accuracy (Optimized): {accuracy_optimized:.4f}")
print(f"F1 Score (Optimized): {f1_optimized:.4f}")

# Evaluate the accelerated model
accuracy_accelerated, f1_accelerated = evaluate_model(accelerated_model, test_loader)
print(f"Accuracy (Accelerated): {accuracy_accelerated:.4f}")
print(f"F1 Score (Accelerated): {f1_accelerated:.4f}")



